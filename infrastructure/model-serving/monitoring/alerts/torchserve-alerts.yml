groups:
  - name: torchserve_alerts
    interval: 30s
    rules:
      # High Latency Alert
      - alert: TorchServeHighLatency
        expr: |
          histogram_quantile(0.95, 
            sum(rate(ts_inference_latency_microseconds_bucket[5m])) by (model_name, le)
          ) / 1000 > 500
        for: 5m
        labels:
          severity: warning
          service: torchserve
          team: ml-platform
        annotations:
          summary: "High inference latency for model {{ $labels.model_name }}"
          description: "95th percentile latency is {{ $value }}ms (threshold: 500ms)"
          dashboard: "https://grafana.betterprompts.com/d/torchserve-overview"

      # Critical Latency Alert
      - alert: TorchServeCriticalLatency
        expr: |
          histogram_quantile(0.95, 
            sum(rate(ts_inference_latency_microseconds_bucket[5m])) by (model_name, le)
          ) / 1000 > 1000
        for: 3m
        labels:
          severity: critical
          service: torchserve
          team: ml-platform
        annotations:
          summary: "Critical inference latency for model {{ $labels.model_name }}"
          description: "95th percentile latency is {{ $value }}ms (threshold: 1000ms)"
          runbook: "https://wiki.betterprompts.com/runbooks/torchserve-latency"

      # High Error Rate
      - alert: TorchServeHighErrorRate
        expr: |
          sum(rate(ts_inference_requests_total{status!="200"}[5m])) by (model_name)
          /
          sum(rate(ts_inference_requests_total[5m])) by (model_name)
          > 0.05
        for: 5m
        labels:
          severity: warning
          service: torchserve
          team: ml-platform
        annotations:
          summary: "High error rate for model {{ $labels.model_name }}"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      # Model Loading Failed
      - alert: TorchServeModelLoadFailed
        expr: ts_model_load_errors_total > 0
        for: 1m
        labels:
          severity: critical
          service: torchserve
          team: ml-platform
        annotations:
          summary: "Model loading failed"
          description: "{{ $value }} model load errors detected"

      # GPU Memory Pressure
      - alert: TorchServeGPUMemoryPressure
        expr: |
          (1 - (DCGM_FI_DEV_FB_FREE / DCGM_FI_DEV_FB_TOTAL)) > 0.9
        for: 5m
        labels:
          severity: warning
          service: torchserve
          team: ml-platform
        annotations:
          summary: "GPU memory pressure on {{ $labels.gpu }}"
          description: "GPU memory usage is {{ $value | humanizePercentage }}"

      # Pod Restart Alert
      - alert: TorchServePodRestart
        expr: |
          increase(kube_pod_container_status_restarts_total{namespace="model-serving", pod=~"torchserve.*"}[30m]) > 0
        labels:
          severity: warning
          service: torchserve
          team: ml-platform
        annotations:
          summary: "TorchServe pod restarted"
          description: "Pod {{ $labels.pod }} has restarted {{ $value }} times"

      # Queue Overflow
      - alert: TorchServeQueueOverflow
        expr: ts_queue_size > 100
        for: 3m
        labels:
          severity: warning
          service: torchserve
          team: ml-platform
        annotations:
          summary: "TorchServe request queue overflow"
          description: "Queue size is {{ $value }} (threshold: 100)"

      # Model Version Drift
      - alert: TorchServeModelVersionDrift
        expr: |
          count(count by (model_name, model_version) (ts_model_version_info)) by (model_name) > 1
        for: 10m
        labels:
          severity: info
          service: torchserve
          team: ml-platform
        annotations:
          summary: "Multiple versions of {{ $labels.model_name }} are running"
          description: "Detected version drift during deployment"

      # Insufficient Replicas
      - alert: TorchServeInsufficientReplicas
        expr: |
          kube_deployment_status_replicas_available{deployment="torchserve", namespace="model-serving"}
          <
          kube_deployment_spec_replicas{deployment="torchserve", namespace="model-serving"} * 0.8
        for: 5m
        labels:
          severity: warning
          service: torchserve
          team: ml-platform
        annotations:
          summary: "Insufficient TorchServe replicas available"
          description: "Only {{ $value }} replicas available (expected: {{ $labels.spec_replicas }})"