{
  "dashboard": {
    "id": null,
    "uid": "torchserve-overview",
    "title": "TorchServe Model Serving Overview",
    "tags": ["torchserve", "ml", "models"],
    "timezone": "browser",
    "schemaVersion": 38,
    "version": 1,
    "refresh": "30s",
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "panels": [
      {
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 0
        },
        "id": 1,
        "targets": [
          {
            "expr": "sum(rate(ts_inference_requests_total[5m])) by (model_name)",
            "legendFormat": "{{ model_name }}",
            "refId": "A"
          }
        ],
        "title": "Inference Requests per Second",
        "type": "timeseries",
        "fieldConfig": {
          "defaults": {
            "unit": "reqps"
          }
        }
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 0
        },
        "id": 2,
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(ts_inference_latency_microseconds_bucket[5m])) by (model_name, le)) / 1000",
            "legendFormat": "{{ model_name }} - p95",
            "refId": "A"
          },
          {
            "expr": "histogram_quantile(0.99, sum(rate(ts_inference_latency_microseconds_bucket[5m])) by (model_name, le)) / 1000",
            "legendFormat": "{{ model_name }} - p99",
            "refId": "B"
          }
        ],
        "title": "Inference Latency (ms)",
        "type": "timeseries",
        "fieldConfig": {
          "defaults": {
            "unit": "ms"
          }
        }
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "gridPos": {
          "h": 8,
          "w": 8,
          "x": 0,
          "y": 8
        },
        "id": 3,
        "targets": [
          {
            "expr": "sum(rate(ts_inference_requests_total{status!=\"200\"}[5m])) by (model_name) / sum(rate(ts_inference_requests_total[5m])) by (model_name) * 100",
            "legendFormat": "{{ model_name }}",
            "refId": "A"
          }
        ],
        "title": "Error Rate (%)",
        "type": "timeseries",
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "yellow",
                  "value": 1
                },
                {
                  "color": "red",
                  "value": 5
                }
              ]
            }
          }
        }
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "gridPos": {
          "h": 8,
          "w": 8,
          "x": 8,
          "y": 8
        },
        "id": 4,
        "targets": [
          {
            "expr": "ts_queue_size",
            "legendFormat": "{{ model_name }}",
            "refId": "A"
          }
        ],
        "title": "Request Queue Size",
        "type": "timeseries",
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "yellow",
                  "value": 50
                },
                {
                  "color": "red",
                  "value": 100
                }
              ]
            }
          }
        }
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "gridPos": {
          "h": 8,
          "w": 8,
          "x": 16,
          "y": 8
        },
        "id": 5,
        "targets": [
          {
            "expr": "1 - (DCGM_FI_DEV_FB_FREE / DCGM_FI_DEV_FB_TOTAL)",
            "legendFormat": "GPU {{ gpu }}",
            "refId": "A"
          }
        ],
        "title": "GPU Memory Usage",
        "type": "timeseries",
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "max": 1,
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "yellow",
                  "value": 0.7
                },
                {
                  "color": "red",
                  "value": 0.9
                }
              ]
            }
          }
        }
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 16
        },
        "id": 6,
        "targets": [
          {
            "expr": "container_cpu_usage_seconds_total{namespace=\"model-serving\", pod=~\"torchserve.*\"}",
            "legendFormat": "{{ pod }}",
            "refId": "A"
          }
        ],
        "title": "CPU Usage by Pod",
        "type": "timeseries"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 16
        },
        "id": 7,
        "targets": [
          {
            "expr": "container_memory_usage_bytes{namespace=\"model-serving\", pod=~\"torchserve.*\"}",
            "legendFormat": "{{ pod }}",
            "refId": "A"
          }
        ],
        "title": "Memory Usage by Pod",
        "type": "timeseries",
        "fieldConfig": {
          "defaults": {
            "unit": "decbytes"
          }
        }
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "gridPos": {
          "h": 4,
          "w": 6,
          "x": 0,
          "y": 24
        },
        "id": 8,
        "targets": [
          {
            "expr": "kube_deployment_status_replicas_available{deployment=\"torchserve\", namespace=\"model-serving\"}",
            "refId": "A"
          }
        ],
        "title": "Available Replicas",
        "type": "stat"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "gridPos": {
          "h": 4,
          "w": 6,
          "x": 6,
          "y": 24
        },
        "id": 9,
        "targets": [
          {
            "expr": "ts_model_version_info",
            "format": "table",
            "refId": "A"
          }
        ],
        "title": "Active Model Versions",
        "type": "table"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "gridPos": {
          "h": 4,
          "w": 6,
          "x": 12,
          "y": 24
        },
        "id": 10,
        "targets": [
          {
            "expr": "sum(increase(ts_inference_requests_total[24h])) by (model_name)",
            "format": "table",
            "refId": "A"
          }
        ],
        "title": "24h Request Count",
        "type": "table"
      },
      {
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "gridPos": {
          "h": 4,
          "w": 6,
          "x": 18,
          "y": 24
        },
        "id": 11,
        "targets": [
          {
            "expr": "up{job=\"torchserve\"}",
            "format": "table",
            "refId": "A"
          }
        ],
        "title": "Service Health",
        "type": "table"
      }
    ],
    "templating": {
      "list": [
        {
          "current": {
            "selected": true,
            "text": "All",
            "value": "$__all"
          },
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "definition": "label_values(ts_inference_requests_total, model_name)",
          "hide": 0,
          "includeAll": true,
          "label": "Model",
          "multi": true,
          "name": "model",
          "query": {
            "query": "label_values(ts_inference_requests_total, model_name)",
            "refId": "PrometheusVariableQueryEditor-VariableQuery"
          },
          "refresh": 1,
          "regex": "",
          "skipUrlSync": false,
          "sort": 0,
          "type": "query"
        }
      ]
    }
  }
}